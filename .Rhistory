select(sample, season_tb)
yrb_meta3 <- yrb_meta2 %>% left_join(., seasons_yrb, by = "sample")
View(yrb_meta3)
wrol_meta <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv")) %>%
mutate(.after = whondrs_id,
dataset = "WROL",
sample_n = as.numeric(whondrs_id),
Date = dmy(sample_date),
site_description = Watershed)
wrol_meta <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv")) %>%
mutate(.after = whondrs_id,
dataset = "WROL",
sample_n = as.numeric(whondrs_id),
Date = dmy(sample_date),
site_description = Watershed) %>%
select(Site, Date, sample_n, Watershed, site_description,
season_tb, wrol_mo_end_member)
wrol_meta <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv")) %>%
mutate(.after = whondrs_id,
dataset = "WROL",
sample_n = as.numeric(whondrs_id),
Date = dmy(sample_date),
site_description = Watershed) %>%
select(Site, Date, sample_n, Watershed, site_description,
season_tb, wrol_mo_end_member)
#Get site, date and sample number from WROL dataset
wrol_meta <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv")) %>%
mutate(.after = whondrs_id,
dataset = "WROL",
sample_n = as.numeric(whondrs_id),
Date = dmy(sample_date),
site_description = Watershed) %>%
select(Site, Date, sample_n, Watershed, site_description,
season_tb, wrol_mo_end_member) %>%
drop_na(sample_n)
#Check for duplicates
wrol_meta %>% count(sample_n) %>% filter(n>1)
#Add the sample names
wrol_meta2 <- sample_ids %>% filter(dataset == "WROL") %>%
left_join(., wrol_meta, by = "sample_n")
View(yrb_meta2)
View(wrol_meta2)
#bind samples together
meta <- bind_rows(wrol_meta2, yrb_meta3)
View(meta)
View(meta)
meta2 <- meta %>%
mutate(Month = month(Date),
season = factor(case_when(Month %in% c(12, 1, 2) ~ "Winter",
Month %in% 3:5 ~ "Spring",
Month %in% 6:8 ~ "Summer",
Month %in% 9:11 ~ "Fall")),
season_n = factor(case_when(Month %in% c(12, 1, 2) ~ 1,
Month %in% 3:5 ~ 2,
Month %in% 6:8 ~ 3,
Month %in% 9:11 ~ 4)))
View(meta2)
saveRDS(meta2, paste0(here, "/output/meta.Rds"))
write_csv(meta2, paste0(here, "/output/meta.csv"))
#clean environment
to_remove <- ls() %>% as_tibble() %>%
filter(str_detect(value, "here", negate = TRUE)) %>%
pull()
rm(list = to_remove)
trans <- read_csv(paste0(here, "/data/WROL-RC2 Transformations Updated 20230828/RC2_WROL_Total_and_Normalized_Transformations_083023.csv"))
#parse sample name into dataset and sample numbers
sample_ids <- trans %>% select(sample) %>%
mutate(dataset = case_when(str_detect(sample, "RC2_") ~ "YRB",
str_detect(sample, "WROL") ~ "WROL"),
sample_n = as.numeric(str_extract(sample, "(?<=_)[^_]+")))
#Check for duplicates
sample_ids %>% group_by(dataset) %>% count() %>% filter(n>1)
#Get site, date and sample number from WROL dataset
wrol_meta <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv")) %>%
mutate(.after = whondrs_id,
dataset = "WROL",
sample_n = as.numeric(whondrs_id),
Date = dmy(sample_date),
site_description = Watershed) %>%
select(Site, Date, sample_n, Watershed, site_description,
season_tb, wrol_mo_end_member) %>%
drop_na(sample_n)
#Check for duplicates
wrol_meta %>% count(sample_n) %>% filter(n>1)
#Add the sample names
wrol_meta2 <- sample_ids %>% filter(dataset == "WROL") %>%
left_join(., wrol_meta, by = "sample_n")
#Get site, sample, date from YRB dataset
yrb_meta <- read_csv(paste0(here, "/data/RC2_Sample_Field_Metadata_krDateEdit20230828.csv")) %>%
mutate(Watershed = "Yakima") %>%
select(Site_ID, Sample_Name, Date, Watershed, Site_Name) %>%
rename("Site" = "Site_ID",
"sample" = "Sample_Name",
"site_description" = "Site_Name")
yrb_meta2 <- sample_ids %>% filter(dataset == "YRB") %>%
left_join(., yrb_meta, by = "sample")
#Check for duplicates
yrb_meta2 %>% count(sample_n) %>% filter(n>1)
#Add manual season classification from Ted B. just for YRB
seasons_yrb <- read_csv(paste0(here, "/data/ancillary chemistry/seasons_yrb.csv")) %>%
select(sample, season_tb)
yrb_meta3 <- yrb_meta2 %>% left_join(., seasons_yrb, by = "sample")
#1.0 Combine metadata tables together
meta <- bind_rows(wrol_meta2, yrb_meta3)
#1.1 Add other season classes ----
meta2 <- meta %>%
mutate(Month = month(Date),
season = factor(case_when(Month %in% c(12, 1, 2) ~ "Winter",
Month %in% 3:5 ~ "Spring",
Month %in% 6:8 ~ "Summer",
Month %in% 9:11 ~ "Fall")),
season_n = factor(case_when(Month %in% c(12, 1, 2) ~ 1,
Month %in% 3:5 ~ 2,
Month %in% 6:8 ~ 3,
Month %in% 9:11 ~ 4)))
View(meta2)
saveRDS(meta2, paste0(here, "/output/meta.Rds"))
write_csv(meta2, paste0(here, "/output/meta.csv"))
#clean environment
to_remove <- ls() %>% as_tibble() %>%
filter(str_detect(value, "here", negate = TRUE)) %>%
pull()
rm(list = to_remove)
#0.0 Setup----
library(here)
here <- here()
library(tidyverse)
library(naniar)
library(rstatix)
library(FactoMineR)
library("devtools")
# install_github("kassambara/factoextra")
library(factoextra)
library(corrplot)
library(ggpubr)
#1.0 Input data----
#load metadata and geospatial dataframes
meta <- readRDS(paste0(here, "/output/meta.Rds"))
View(meta)
sites <- meta$Site %>% as_factor() #52 Sites
geospat <- read_csv(paste0(here, "/data/geospat/Geospatial_all_YRB_WROL_2023-05-01.csv"))
sites_missing <- setdiff(sites, geospat$site)
geospat %>% count(site) %>% filter(n>1) #no duplicated site names
glimpse(geospat)
#Add additional site info
geospat_add <- read_csv(paste0(here, "/data/geospat/geospat_additional_sites_20230915.csv"))
View(geospat_add)
#Join
geospat2 <- bind_rows(geospat, geospat_add)
View(geospat2)
#Select targeted geospatial variables
geospat3 <- geospat2 %>%
select(site, streamorde, ElevWs, #mean elevation in meters
TmeanSite, #mean 30 year temperature in Celsius for selected years
PrecipSite, #mean 30 year precipitation (mm)
RunoffWs, #mean runoff (mm)
areasqkm, CatAreaSqKm, WsAreaSqKm, slope,
PctOw2019Ws, #percent open water in 2019
PctMxFst2019Ws, #percent mixed forest
PctDecid2019Ws, #percent deciduous forest
PctConif2019Ws, #percent deciduous forest
PctCrop2019Ws, #percent crop land use
PctWdWet2019Ws, #percent woody wetland cover
PctHbWet2019Ws, #percent herbaceous wetland cover
PctUrbHi2019Ws, #percent high intensity urban development
PctImp2019Ws, #mean percent impervious land cover for 2019
PopDen2010Ws, #percent population density in 2010 Census
RdDensWs, #mean road density (km/sq.km)
KffactWs, #mean soil erodibility factor (Kf, unitless)
WWTPAllDensWs, #waster water treatment plant density (#/sq.km)
OmWs, #mean percent organic matter soil content
DamDensWs, #dams/sq.km
HydrlCondWs, #mean hydraulic conductivity of surface geologic (micrometers/second)
ClayWs, #mean percent clay content of soils
SandWs, #mean percent sand content of soils
PermWs, #mean permeability of soils (cm/hour)
)
glimpse(geospat3)
geospat4 <- geospat3 %>%
convert_as_factor(vars = c("site"#, "streamorde"
)) %>%
bind_rows(., geospat_filled) %>%
left_join(., meta %>% select(Site, Watershed), by = c("site" = "Site")) %>%
select(Watershed, site, everything()) %>%
distinct(site, .keep_all = TRUE) %>%
rowid_to_column(var = "rowid") %>%
column_to_rownames(var = "site") %>%
drop_na(streamorde)
geospat4 <- geospat3 %>%
convert_as_factor(vars = c("site"#, "streamorde"
)) %>%
left_join(., meta %>% select(Site, Watershed), by = c("site" = "Site")) %>%
select(Watershed, site, everything()) %>%
distinct(site, .keep_all = TRUE) %>%
rowid_to_column(var = "rowid") %>%
column_to_rownames(var = "site") %>%
drop_na(streamorde)
gg_miss_var(geospat4) #No missing geospatial data per site
#2.0 PCA----
##2.1 Define input variables----
vars_active <- names(geospat4[,3:length(geospat4)])
vars_supl <- names(geospat4[,1:2])
vars_watershed <- geospat4 %>%
select(streamorde,
ElevWs, #mean elevation in meters
TmeanSite, #mean 30 year temperature in Celsius for selected years
PrecipSite, #mean 30 year precipitation (mm)
RunoffWs, #mean runoff (mm)
# areasqkm, CatAreaSqKm,
WsAreaSqKm,
# slope,
# RdDensWs, #mean road density (km/sq.km)
# KffactWs, #mean soil erodibility factor (Kf, unitless)
OmWs, #mean percent organic matter soil content
# DamDensWs, #dams/sq.km
# HydrlCondWs, #mean hydraulic conductivity of surface geologic (micrometers/second)
ClayWs, #mean percent clay content of soils
SandWs, #mean percent sand content of soils
# PermWs, #mean permeability of soils (cm/hour)
PctDecid2019Ws, #percent deciduous forest
PctConif2019Ws #percent deciduous forest
) %>% names()
vars_lulc <- geospat4 %>%
select(PctConif2019Ws, #percent deciduous forest,
PctOw2019Ws, #percent open water in 2019
PctMxFst2019Ws, #percent mixed forest
PctDecid2019Ws, #percent deciduous forest
PctCrop2019Ws, #percent crop land use
PctWdWet2019Ws, #percent woody wetland cover
PctHbWet2019Ws, #percent herbaceous wetland cover
PctUrbHi2019Ws, #percent high intensity urban development
# PctImp2019Ws, #mean percent impervious land cover for 2019
# PopDen2010Ws, #percent population density in 2010 Census
# RdDensWs, #mean road density (km/sq.km)
# WWTPAllDensWs, #waster water treatment plant density (#/sq.km)
OmWs, #mean percent organic matter soil content
# DamDensWs, #dams/sq.km
# HydrlCondWs, #mean hydraulic conductivity of surface geologic (micrometers/second)
ClayWs, #mean percent clay content of soils
SandWs, #mean percent sand content of soils
# PermWs, #mean permeability of soils (cm/hour)
) %>% names()
#plot QQ plots to visually inspect for normality
geospat4 %>%
pivot_longer(all_of(vars_active)) %>%
group_by(name) %>%
ggqqplot("value", facet.by = "name", scale = "free")
geospat4 %>%
pivot_longer(all_of(vars_watershed)) %>%
group_by(name) %>%
ggqqplot("value", facet.by = "name", scale = "free")
geospat4 %>%
pivot_longer(all_of(vars_lulc)) %>%
group_by(name) %>%
ggqqplot("value", facet.by = "name", scale = "free")
#Assess overall correlations
corr <- cor(geospat4 %>% select(all_of(vars_active)))
corrplot(corr, diag = TRUE,
title = "Correlations",
mar=c(0,0,1,0), tl.col = "black")
#get column indices for PCA input
pca_vars_active <- match(vars_active, names(geospat4))
pca_vars_supl <- match(vars_supl, names(geospat4))
pca_vars_watershed <- match(vars_watershed, names(geospat4))
pca_vars_lulc <- match(vars_lulc, names(geospat4))
##2.2 Run PCA----
rownames(geospat4)
res.pca_active <- PCA(X = geospat4, scale.unit = TRUE,
quali.sup = pca_vars_supl)
res.pca_watershed <-PCA(X = geospat4 %>%
select(all_of(c(pca_vars_supl, pca_vars_watershed))),
scale.unit = TRUE,
quali.sup = pca_vars_supl)
res.pca_lulc <-PCA(X = geospat4 %>%
select(all_of(c(pca_vars_supl, pca_vars_lulc))),
scale.unit = TRUE,
quali.sup = pca_vars_supl)
res.pca_lulc <-PCA(X = geospat4 %>%
select(all_of(c(pca_vars_supl, pca_vars_lulc))),
scale.unit = TRUE,
quali.sup = pca_vars_supl)
res.pca_lulc_west <-PCA(X = geospat4 %>%
filter(Watershed != "Connecticut") %>%
select(all_of(c(pca_vars_supl, pca_vars_lulc))),
scale.unit = TRUE,
quali.sup = pca_vars_supl)
#Extract the results for individuals and variables, respectively
#Visualize the results individuals and variables, respectively.
## Principal Component Analysis Results for variables
##   Name       Description
## 1 "$coord"   "Coordinates for the variables"
## 2 "$cor"     "Correlations between variables and dimensions"
## 3 "$cos2"    "Cos2 for the variables"
## 4 "$contrib" "contributions of the variables"
pca_var_active <- get_pca_var(res.pca_active)
pca_var_watershed <- get_pca_var(res.pca_watershed)
pca_var_lulc <- get_pca_var(res.pca_lulc)
pca_var_lulc_west <- get_pca_var(res.pca_lulc_west)
#Visualize Individuals
pca_ind_active <- get_pca_ind(res.pca_active)$coord
pca_ind_watershed <- get_pca_ind(res.pca_watershed)
pca_ind_lulc <- get_pca_ind(res.pca_lulc)
pca_ind_lulc_west <- get_pca_ind(res.pca_lulc_west)
##2.4Extract PCA loadings----
geospat5 <- pca_ind_lulc$coord %>% as_tibble() %>%
rownames_to_column(var = "rowid") %>%
mutate(across(rowid, as.integer)) %>%
left_join(geospat4 %>% rownames_to_column("site"), ., by = "rowid") %>%
select(-c(rowid, Dim.3, Dim.4, Dim.5)) %>%
rename("lulc_pca1" = "Dim.1",
"lulc_pca2" = "Dim.2") %>%
mutate(decid_conif = PctDecid2019Ws/PctConif2019Ws,
clay_sand = ClayWs/SandWs,
claysand_decidconif = clay_sand/decid_conif)
#Plot principal components
ggplot(geospat5) +
geom_point(aes(x= lulc_pca1, y= lulc_pca2, color = Watershed),
size = 4) +
theme_bw()
View(geospat5)
#number of types
k <- geospat5 %>% select(contains("Pct")) %>% colnames() %>% length()
#Adapted Shannon Diversity Evenness
#-sum(proportion type*log proportion for each site)/log(k)
geospat6 <- geospat5 %>%
# rownames_to_column("site") %>%
select(Watershed, site, contains("Pct")) %>%
pivot_longer(cols = contains("Pct")) %>%
mutate(log_pct = case_when(value == 0 ~ 0,
TRUE ~ log(value/100)),
pct_log_pct = value/100 * log_pct) %>%
group_by(site) %>%
mutate(lulc_H = -sum(pct_log_pct),
lulc_evenness = lulc_H/log(k)) %>%
ungroup() %>%
distinct(site, .keep_all = TRUE) %>%
select(site, lulc_H, lulc_evenness) %>%
left_join(geospat4, ., by = "site")
#Adapted Shannon Diversity Evenness
#-sum(proportion type*log proportion for each site)/log(k)
geospat6 <- geospat5 %>%
# rownames_to_column("site") %>%
select(Watershed, site, contains("Pct")) %>%
pivot_longer(cols = contains("Pct")) %>%
mutate(log_pct = case_when(value == 0 ~ 0,
TRUE ~ log(value/100)),
pct_log_pct = value/100 * log_pct) %>%
group_by(site) %>%
mutate(lulc_H = -sum(pct_log_pct),
lulc_evenness = lulc_H/log(k)) %>%
ungroup() %>%
distinct(site, .keep_all = TRUE) %>%
select(site, lulc_H, lulc_evenness) %>%
left_join(geospat5, ., by = "site")
View(geospat6)
ggplot(geospat6) +
geom_boxplot(aes(x=Watershed, y= lulc_evenness)) +
geom_jitter(aes(x=Watershed, y= lulc_evenness, color = site), size = 3) +
theme_bw()
ggplot(geospat6) +
geom_boxplot(aes(x=Watershed, y= lulc_H)) +
geom_jitter(aes(x=Watershed, y= lulc_H, color = site), size = 3) +
theme_bw()
ggplot(geospat6) +
geom_point(aes(x=SandWs, y= ClayWs, color = OmWs, shape = Watershed),
size = 4) +
theme_bw()
ggplot(geospat6) +
geom_point(aes(y=ClayWs/SandWs, x= PctConif2019Ws/PctDecid2019Ws, color = Watershed),
size = 4) +
scale_x_log10() +
# geom_point(aes(x=PctConif2019Ws, y= SandWs, color = Watershed),
#            size = 4) +
theme_bw()
#5.0 SAVE
saveRDS(geospat6 %>% select(-Watershed),
paste0(here, "/output/geospat/geospat_indices.Rds"))
View(geospat6)
#clean environment
to_remove <- ls() %>% as_tibble() %>%
filter(str_detect(value, "here", negate = TRUE)) %>%
pull()
rm(list = to_remove)
#0.0 Setup----
library(here)
here <- here()
library(tidyverse)
library(lubridate)
#1.0 Input data----
#Load various sources of data and assign sample identification information
meta <- readRDS(paste0(here, "/output/meta.Rds"))
##1.2 Load geospatial - complete for all sites (ID = Site ID)
geospat <- readRDS(paste0(here, "/output/geospat/geospat_indices.Rds"))
##1.3 Load FT ICR-MS indices (ID = sample name)
#Mass difference transformation analysis results
# trans <- read_csv(paste0(here, "/output/transformation analysis/RC2_WROL_Total_and_Normalized_Transformations_082823.csv"))
trans <- read_csv(paste0(here, "/data/WROL-RC2 Transformations Updated 20230828/RC2_WROL_Total_and_Normalized_Transformations_083023.csv"))
#Compound Class Summary
compound_class <- read_csv(paste0(here, "/data/WROL-RC2 Transformations Updated 20230828/WROL_RC2_Compound_Class_Summary.csv")) %>% rename("sample" = "...1")
#Elemental Composition Summary
elemental_comp <- read_csv(paste0(here, "/data/WROL-RC2 Transformations Updated 20230828/WROL_RC2_Elemental_Composition_Summary.csv")) %>% rename("sample" = "...1")
#Molecular Indices Summary
mol_info <- read_csv(paste0(here, "/data/WROL-RC2 Transformations Updated 20230828/WROL_RC2_MolInfo_Summary.csv")) %>% rename("sample" = "...1")
indices <- left_join(trans, mol_info, by = "sample") %>%
left_join(., compound_class, by = "sample") %>%
left_join(., elemental_comp, by = "sample") %>%
select(-contains(c(".median", ".sd", ";", "NA"))) %>%
mutate(across(Lignin:CHOSP, .fns= ~./number.of.peaks, .names= "{.col}_norm"))
glimpse(indices)
chem_wrol <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv"))
View(chem_wrol)
chem_wrol <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv")) %>%
drop_na(wrol_sample_id)
chem_wrol <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv")) %>%
drop_na(wrol_sample_id) %>%
mutate(.after = whondrs_id,
dataset = "WROL",
sample_n = as.numeric(whondrs_id),
date = dmy(sample_date)) %>%
drop_na(sample_n)
View(chem_wrol)
chem_wrol <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv")) %>%
drop_na(wrol_sample_id) %>%
mutate(.after = whondrs_id,
dataset = "WROL",
sample_n = as.numeric(whondrs_id),
date = dmy(sample_date)) %>%
drop_na(sample_n) %>%
left_join(., meta %>% select(sample, dataset, sample_n),
by = c("dataset", "sample_n"))
chem_wrol <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv")) %>%
drop_na(wrol_sample_id) %>%
mutate(.after = whondrs_id,
dataset = "WROL",
sample_n = as.numeric(whondrs_id),
date = dmy(sample_date)) %>%
drop_na(sample_n) %>%
left_join(., meta %>% select(sample, dataset, sample_n),
by = c("dataset", "sample_n")) %>%
drop_na(sample) %>% #Some HRMS samples excluded due to poor mass calibration
rename("DOC_mgL" = "DOC",
"TN_mgL" = "TN")
##1.4 Load Other Chemistry  (ID = dataset, sample #)
##' WROL
chem_wrol <- read_csv(paste0(here, "/data/ancillary chemistry/wrol_chem_20230915.csv")) %>%
drop_na(wrol_sample_id) %>%
mutate(.after = whondrs_id,
dataset = "WROL",
sample_n = as.numeric(whondrs_id),
date = dmy(sample_date)) %>%
drop_na(sample_n) %>%
left_join(., meta %>% select(sample, dataset, sample_n),
by = c("dataset", "sample_n")) %>%
drop_na(sample) %>% #Some HRMS samples excluded due to poor mass calibration
rename("DOC_mgL" = "DOC",
"TN_mgL" = "TN") %>%
select(sample, DOC_mgL, TN_mgL, t, c, uva254, S275295, suva254) %>%
mutate(across(.cols = DOC_mgL:suva254, .fns = as.numeric))
#Check for duplicates
chem_wrol %>% count(sample) %>% filter(n>1)
View(meta)
##' Yakima from RC2 data release (ID = Sample Name)
chem_yrb <- read_csv(paste0(here, "/data/ancillary chemistry/RC2_NPOC_TN_DIC_TSS_Ions_Summary_2021-2022.csv"),
skip = 2) %>%
slice(-c(1:11, 205)) %>%
select(-c(Field_Name, Material, Mean_Missing_Reps)) %>%
rename("sample" = "Sample_Name",
"DOC_mgL" = "Mean_00681_NPOC_mg_per_L_as_C",
"TN_mgL" = "Mean_00602_TN_mg_per_L_as_N",
"TSS_mgL" = "Mean_00530_TSS_mg_per_L") %>%
select(sample, DOC_mgL, TN_mgL, TSS_mgL) %>%
mutate(across(.cols = DOC_mgL:TSS_mgL, .fns = as.numeric)) %>%
mutate(across(.cols = DOC_mgL:TSS_mgL,
.fns = ~case_when(.x < 0 ~ NA_real_,
TRUE ~ .x)))
chem <- bind_rows(chem_wrol, chem_yrb)
View(chem)
##1.5 Load travel time
##' WROL sites (ID = dataset, sample_n)
##'
tt <- read_csv(paste0(here, "/data/travel time/meta_tt_fromTedB_18Aug2023_tb.csv")) %>%
select(sample, tt_hr, q_daily_cms) %>%
mutate(across(.cols = tt_hr:q_daily_cms, .fns = as.numeric))
View(tt)
dat <- list(meta = meta,
geospat = geospat,
indices = indices,
chem = chem,
tt = tt)
saveRDS(dat, paste0(here, "/output/dat_compiled_ls.Rds"))
#3.0 Merge data wide ----
list2env(x=readRDS(paste0(here, "/output/dat_compiled_ls.Rds")),
envir = globalenv())
dat <- inner_join(meta, geospat, by = c("Site" = "site")) %>%
left_join(., indices, by = "sample") %>%
left_join(., tt, by = "sample") %>%
left_join(., chem, by = "sample")
glimpse(dat)
View(dat)
#Add additional calculated variables
dat2 <- dat %>%
mutate(.after= q_daily_cms,
q_daily_mm = q_daily_cms/WsAreaSqKm/1000)
glimpse(dat2)
#SAVE
saveRDS(dat2, paste0(here, "/output/dat_wide.Rds"))
write_csv(dat2, paste0(here, "/output/dat_wide.csv"))
#clean environment
to_remove <- ls() %>% as_tibble() %>%
filter(str_detect(value, "here", negate = TRUE)) %>%
pull()
rm(list = to_remove)
